{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9992a-a6bd-4e52-95bf-03f3bbaf09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import itertools as it\n",
    "from env import *\n",
    "import pandas as pd\n",
    "import random\n",
    "from BR_agent import *\n",
    "from MO_agent import *\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54283c19-da64-4a81-99f8-e66645891474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e=tiger_POMDP_env(read_config=True,config_address='./tiger.json',parameters=None)\n",
    "num_to_act=dict(zip(list(e.actions.values()),list(e.actions.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cca18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(list(e.actions.values()),list(e.actions.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b971c-d6ac-4c22-811a-aa89d1448196",
   "metadata": {},
   "source": [
    "### Check similarity of methods' actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6cf5aa-015d-4510-b781-b5daae011d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "E=[-1.5,-0.2,-0.1,0.1,0.2,1.5]\n",
    "W=[-1,0,1,2]\n",
    "inits=[[1,0],[0.9,0.1],[0.75,0.25],[0.5,0.5]]\n",
    "initial_observation=0\n",
    "\n",
    "loop_counts=0\n",
    "for max_depth in range(1,3):\n",
    "    corr_count=0\n",
    "    all_count=0\n",
    "    loop_counts+=1\n",
    "    print(loop_counts)\n",
    "    for e1 in E:\n",
    "        for e2 in E:\n",
    "            exp_vorfaktoren=[e1,e2]\n",
    "            for w1 in W:\n",
    "                for w2 in W:\n",
    "                    exp_weights=[w1,w2]\n",
    "                    #print(exp_vorfaktoren, exp_weights)\n",
    "                    for init in inits:\n",
    "                        ag2=Multi_Variate_agent(environment=e, planning_depth=max_depth, partitioning_chunk_number=10,agent_mode='cheating')\n",
    "                        ag2.reset()\n",
    "                        x_map2,M2,F2,G2,X2,value_function2,all_theta2=ag2.pre_planning(exp_vorfaktoren=exp_vorfaktoren,exp_weights=exp_weights, initial_theta=init,initial_observation=initial_observation)\n",
    "                        v2,a2,q2,vf2=ag2.value_iteration()\n",
    "                        \n",
    "                        \n",
    "                        ag=Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=2,max_iterations=max_depth)\n",
    "                        b_S,b_r,b_map,b_q_func,b_value_function,b_action_func=ag.continious_optimized_planning(initial_mu_state=init,initial_observation=initial_observation,initial_wealth=0,exp_weights=exp_weights,exp_vorfaktoren=exp_vorfaktoren)\n",
    "                        \n",
    "                        #m_r,m_map, m_q_func,m_value_function,m_action_func=MO_cheating(env_dynamics=e,planning_depth=max_depth,exp_vorfaktoren=exp_vorfaktoren,exp_weights=exp_weights ,initial_theta=init,initial_observation=0,rounding_prec_coeff=10000)\n",
    "                        #b_S,b_r,b_map,b_q_func,b_value_function,b_action_func=Beurele_cheating(env_dynamics=e,planning_depth=max_depth,exp_vorfaktoren=exp_vorfaktoren,exp_weights=exp_weights ,initial_mu_state=init,initial_wealth=0,comparison_precision=1.0e-5)\n",
    "                        \n",
    "                        mx_init=init*len(init)\n",
    "                        mx_init.append(0)\n",
    "                        mr_init=[0]*len(init)\n",
    "                        m_init=(tuple(mx_init),tuple(mr_init),0)\n",
    "                        b_init=(initial_observation,tuple(init),0)\n",
    "                        all_count+=1\n",
    "                        if a2[0][tuple(m_init)]==b_action_func[0][b_init]:\n",
    "                            corr_count+=1\n",
    "                        else:\n",
    "                            #pass\n",
    "                            print(q2[0][tuple(m_init)], b_q_func[0][tuple(init)],'           ',a2[0][tuple(m_init)],b_action_func[0][0][tuple(init)],'    ||     exp:',exp_vorfaktoren,exp_weights,init)\n",
    "    print('--------------------------')                        \n",
    "    print(all_count-corr_count,all_count,str(corr_count*100/all_count)+'%')\n",
    "    print('=====================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d0fa19-8032-4d5a-af18-7b43f4dcf739",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69fe91-7206-445d-be76-e0cd24604ea0",
   "metadata": {},
   "source": [
    "### BR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d53f53-c1f0-4e70-b0cb-8fc3886d5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth=4\n",
    "planning_depth=max_depth\n",
    "exp_vorfaktoren=[-1,0.1]\n",
    "exp_weights=[-1,1]\n",
    "initial_Mu=[0.5,0.5]\n",
    "initial_observation=0\n",
    "initial_wealth=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d286fdd2-48fa-434b-a4c6-9bacc7bd5122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ta=time.time()\n",
    "###############    Environment\n",
    "\n",
    "e.reset()\n",
    "initial_observation=e.current_state\n",
    "observation=initial_observation\n",
    "state=e.current_state\n",
    "##############       Agent\n",
    "# Create the agent\n",
    "ag=Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=2,max_iterations=max_depth)\n",
    "\n",
    "\n",
    "# Run its value iteration\n",
    "b_S,b_reachables,b_map,b_q_func,b_value_function,b_action_func=ag.continious_optimized_planning(initial_mu_state=initial_Mu,initial_observation=initial_observation,initial_wealth=0,exp_weights=exp_weights,exp_vorfaktoren=exp_vorfaktoren)\n",
    "statesused=0\n",
    "for i in range(max_depth+1):\n",
    "    statesused+=len(b_reachables[i])\n",
    "print(\"depth:\",max_depth, \"states used:\", statesused)\n",
    "# reset the agent \n",
    "ag.ch_reset(initial_mu_state=initial_Mu,initial_observation=initial_observation,initial_wealth=initial_wealth)\n",
    "\n",
    "tb=time.time()\n",
    "print(\"time needed:\", tb-ta)\n",
    "\n",
    "#############         Simulation\n",
    "\n",
    "for t in range(planning_depth):\n",
    "    \n",
    "    print('t=',t)\n",
    "    print('------')\n",
    "    print('state:',state,e.states[state])\n",
    "    print('last_observation:',observation)\n",
    "    print('-------------------------------')\n",
    "    print('')\n",
    "    print('current internal state:',ag.current_internal_state)\n",
    "    print('')\n",
    "    \n",
    "    # agent select the action\n",
    "    action,value_of_action,_=ag.ch_do_action()\n",
    "    \n",
    "    #environment feedback\n",
    "    t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "    \n",
    "    # agent update\n",
    "    new_x=ag.ch_update_agent(new_observation=observation) \n",
    "    \n",
    "    print('             action:',num_to_act[action])  \n",
    "    print ('             **' )\n",
    "    print('reward:',reward,'new observation:',observation)\n",
    "    print('')\n",
    "    print('new internal (Mu) state:', ag.current_internal_state)\n",
    "    print('')\n",
    "    print('============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183b795-fae6-4f92-b896-0e30ca3f8ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd8367a1-f1c5-4a74-a2b1-ec83ecef9064",
   "metadata": {},
   "source": [
    "### Multi Variate (Multi Objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd9273-fcff-42ca-9f10-205decf268b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_depth=5\n",
    "partitioning_chunk_number=1000\n",
    "# exp_vorfaktoren=[-1,0.1]\n",
    "# exp_weights=[-1,1]\n",
    "exp_vorfaktoren=[1,-1]\n",
    "exp_weights=[0.5,-0.5]\n",
    "initial_theta=[0.5,0.5]\n",
    "initial_observation=0\n",
    "modes=['discrete_optimized','naive','cheating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f0a997-8f36-40df-81b0-1ab59bd1348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############    Environment\n",
    "t0 = time.time()\n",
    "e.reset()\n",
    "initial_observation=e.current_state\n",
    "observation=initial_observation\n",
    "state=e.current_state\n",
    "\n",
    "##############     Agent \n",
    "\n",
    "ag=Multi_Variate_agent(environment=e, planning_depth=planning_depth, partitioning_chunk_number=partitioning_chunk_number,agent_mode='cheating')\n",
    "ag.reset()\n",
    "mo_x_map,mo_M,mo_F,mo_G,mo_reachables,mo_value_function_zero,mo_all_thetas=ag.pre_planning(exp_vorfaktoren=exp_vorfaktoren,exp_weights=exp_weights, initial_theta=initial_theta,initial_observation=initial_observation,initial_wealth=0)\n",
    "mo_value_func,mo_action_func,mo_q_func,vf2=ag.value_iteration()\n",
    "t1 = time.time()\n",
    "statesused=0\n",
    "for i in range(planning_depth+1):\n",
    "    statesused+=len(mo_reachables[i])\n",
    "print(\"depth:\",planning_depth, \"states used:\", statesused)\n",
    "print(\"time needed:\", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf70547-8ff5-4e77-bc1c-03ea908556c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "##############       Simulation\n",
    "for t in range(planning_depth):\n",
    "    \n",
    "    print('t=',t)\n",
    "    print('------')\n",
    "    print('state:',state,e.states[state])\n",
    "    print('last_observation:',observation)\n",
    "    print('-------------------------------')\n",
    "    print('')\n",
    "    print('current internal state:',ag.current_internal_state)\n",
    "    print('')\n",
    "    \n",
    "    # agent select the action\n",
    "    action,value_of_action=ag.do_action()\n",
    "    \n",
    "    #environment feedback\n",
    "    t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "    \n",
    "    # agent update\n",
    "    new_x=ag.update_agent(new_observation=observation) \n",
    "    \n",
    "    print('             action:',num_to_act[action])  \n",
    "    print ('             **' )\n",
    "    print('reward:',reward,'new observation:',observation)\n",
    "    print('')\n",
    "    print('new internal state:', ag.current_internal_state)\n",
    "    print('')\n",
    "    print('============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cbb3d-cbd3-4fc2-b5ab-3c500ffd5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_reachables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88198f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_reachables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7127c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth=3\n",
    "planning_depth=max_depth\n",
    "exp_vorfaktoren=[-1,0.1]\n",
    "exp_weights=[-1,1]\n",
    "initial_Mu=[0.5,0.5]\n",
    "initial_observation=0\n",
    "initial_wealth=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta=time.time()\n",
    "###############    Environment\n",
    "\n",
    "e.reset()\n",
    "initial_observation=e.current_state\n",
    "observation=initial_observation\n",
    "state=e.current_state\n",
    "##############       Agent\n",
    "# Create the agent\n",
    "ag=Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=2,max_iterations=max_depth)\n",
    "\n",
    "\n",
    "# Run its value iteration\n",
    "b_S,b_reachables,b_map,b_q_func,b_value_function,b_action_func=ag.continious_optimized_planning(initial_mu_state=initial_Mu,initial_observation=initial_observation,initial_wealth=0,exp_weights=exp_weights,exp_vorfaktoren=exp_vorfaktoren)\n",
    "statesused=0\n",
    "for i in range(max_depth+1):\n",
    "    statesused+=len(b_reachables[i])\n",
    "print(\"depth:\",max_depth, \"states used:\", statesused)\n",
    "# reset the agent \n",
    "ag.ch_reset(initial_mu_state=initial_Mu,initial_observation=initial_observation,initial_wealth=initial_wealth)\n",
    "\n",
    "tb=time.time()\n",
    "print(\"time needed:\", tb-ta)\n",
    "\n",
    "#############         Simulation\n",
    "\n",
    "for t in range(planning_depth):\n",
    "    \n",
    "    print('t=',t)\n",
    "    print('------')\n",
    "    print('state:',state,e.states[state])\n",
    "    print('last_observation:',observation)\n",
    "    print('-------------------------------')\n",
    "    print('')\n",
    "    print('current internal state:',ag.current_internal_state)\n",
    "    print('')\n",
    "    \n",
    "    # agent select the action\n",
    "    action,value_of_action,_=ag.ch_do_action()\n",
    "    \n",
    "    #environment feedback\n",
    "    t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "    \n",
    "    # agent update\n",
    "    new_x=ag.ch_update_agent(new_observation=observation) \n",
    "    \n",
    "    print('             action:',num_to_act[action])  \n",
    "    print ('             **' )\n",
    "    print('reward:',reward,'new observation:',observation)\n",
    "    print('')\n",
    "    print('new internal (Mu) state:', ag.current_internal_state)\n",
    "    print('')\n",
    "    print('============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd72a90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################## MULTIVARIATE DATA GATHERING ################################################\n",
    "tx=time.time()\n",
    "loops=50\n",
    "\n",
    "results={1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[],9:[],10:[]}\n",
    "comptime={1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[],9:[],10:[]}\n",
    "for i in range(1,7):\n",
    "    \n",
    "        for j in range(loops):\n",
    "\n",
    "            planning_depth=i\n",
    "            partitioning_chunk_number=1000\n",
    "#           exp_vorfaktoren=[1,-1]\n",
    "#           exp_weights=[0.5,-0.5]\n",
    "            exp_vorfaktoren=[-1,0.1]\n",
    "            exp_weights=[-1,1]\n",
    "            initial_theta=[0.5,0.5]\n",
    "            initial_observation=0\n",
    "            modes=['discrete_optimized','naive','cheating']\n",
    "\n",
    "            ###############    Environment\n",
    "            t0 = time.time()\n",
    "            e.reset()\n",
    "            initial_observation=e.current_state\n",
    "            observation=initial_observation\n",
    "            state=e.current_state\n",
    "\n",
    "            ##############     Agent \n",
    "\n",
    "            ag=Multi_Variate_agent(environment=e, planning_depth=planning_depth, partitioning_chunk_number=partitioning_chunk_number,agent_mode='cheating')\n",
    "            ag.reset()\n",
    "            mo_x_map,mo_M,mo_F,mo_G,mo_reachables,mo_value_function_zero,mo_all_thetas=ag.pre_planning(exp_vorfaktoren=exp_vorfaktoren,exp_weights=exp_weights, initial_theta=initial_theta,initial_observation=initial_observation,initial_wealth=0)\n",
    "            mo_value_func,mo_action_func,mo_q_func,vf2=ag.value_iteration()\n",
    "            t1 = time.time()\n",
    "            statesused=0\n",
    "            for i in range(planning_depth+1):\n",
    "                statesused+=len(mo_reachables[i])\n",
    "#             print(\"depth:\",planning_depth, \"states used:\", statesused)\n",
    "#             print(\"time needed:\", t1-t0)\n",
    "            comptime[i].append(t1-t0)\n",
    "\n",
    "            ##############       Simulation\n",
    "            total_reward=0\n",
    "            for t in range(planning_depth):\n",
    "\n",
    "#                 print('t=',t)\n",
    "#                 print('------')\n",
    "#                 print('state:',state,e.states[state])\n",
    "#                 print('last_observation:',observation)\n",
    "#                 print('-------------------------------')\n",
    "#                 print('')\n",
    "#                 print('current internal state:',ag.current_internal_state)\n",
    "#                 print('')\n",
    "\n",
    "                # agent select the action\n",
    "                action,value_of_action=ag.do_action()\n",
    "#                 if action==\"open_right_low\" or action==\"open_left_low\":\n",
    "#                     low+=1\n",
    "#                 elif action==\"open_right_high\" or action==\"open_left_high\":\n",
    "#                     high+=1\n",
    "#                 elif action==\"listen\"\n",
    "#                     listen+=1\n",
    "\n",
    "                #environment feedback\n",
    "                t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "                total_reward+=reward\n",
    "\n",
    "                # agent update\n",
    "                new_x=ag.update_agent(new_observation=observation) \n",
    "\n",
    "\n",
    "#                 print('             action:',num_to_act[action])  \n",
    "#                 print ('             **' )\n",
    "#                 print('reward:',reward,'new observation:',observation)\n",
    "#                 print('')\n",
    "#                 print('new internal state:', ag.current_internal_state)\n",
    "#                 print('')\n",
    "#                 print('============================')\n",
    "#                 print('total_reward', total_reward)\n",
    "\n",
    "            results[i].append(total_reward)\n",
    "        print(i)\n",
    "\n",
    "print(\"results\", results)\n",
    "print(\"=================================\")\n",
    "print(\"time\",comptime)\n",
    "ty=time.time()\n",
    "print(\"----------------------\")\n",
    "print(\"total time needed for\", loops, \"loops\", ty-tx, \"seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd82cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## BAUERLE DATA GATHERING ################################################\n",
    "\n",
    "\n",
    "results_b={1:[],2:[],3:[]}\n",
    "efficiency_b={1:[],2:[],3:[]}\n",
    "\n",
    "for i in range(1,4):\n",
    "    for j in range(2):\n",
    "        total_reward=0\n",
    "        max_depth=i\n",
    "        planning_depth=max_depth\n",
    "        exp_vorfaktoren=[1,-1]\n",
    "        exp_weights=[0.5,-0.5]\n",
    "        initial_Mu=[0.5,0.5]\n",
    "        initial_observation=0\n",
    "        initial_wealth=0\n",
    "\n",
    "        ta=time.time()\n",
    "        ###############    Environment\n",
    "\n",
    "        e.reset()\n",
    "        initial_observation=e.current_state\n",
    "        observation=initial_observation\n",
    "        state=e.current_state\n",
    "        ##############       Agent\n",
    "        # Create the agent\n",
    "        ag=Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=2,max_iterations=max_depth)\n",
    "\n",
    "\n",
    "        # Run its value iteration\n",
    "        b_S,b_reachables,b_map,b_q_func,b_value_function,b_action_func=ag.continious_optimized_planning(initial_mu_state=initial_Mu,initial_observation=initial_observation,initial_wealth=0,exp_weights=exp_weights,exp_vorfaktoren=exp_vorfaktoren)\n",
    "        statesused=0\n",
    "        for i in range(max_depth+1):\n",
    "            statesused+=len(b_reachables[i])\n",
    "        print(\"depth:\",max_depth, \"states used:\", statesused)\n",
    "        # reset the agent \n",
    "        ag.ch_reset(initial_mu_state=initial_Mu,initial_observation=initial_observation,initial_wealth=initial_wealth)\n",
    "\n",
    "        tb=time.time()\n",
    "        print(\"time needed:\", tb-ta)\n",
    "        efficiency[i].append(tb-ta)\n",
    "\n",
    "        #############         Simulation\n",
    "\n",
    "        for t in range(planning_depth):\n",
    "\n",
    "            print('t=',t)\n",
    "            print('------')\n",
    "            print('state:',state,e.states[state])\n",
    "            print('last_observation:',observation)\n",
    "            print('-------------------------------')\n",
    "            print('')\n",
    "            print('current internal state:',ag.current_internal_state)\n",
    "            print('')\n",
    "\n",
    "            # agent select the action\n",
    "            action,value_of_action,_=ag.ch_do_action()\n",
    "\n",
    "            #environment feedback\n",
    "            t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "            total_reward+=reward\n",
    "\n",
    "            # agent update\n",
    "            new_x=ag.ch_update_agent(new_observation=observation) \n",
    "\n",
    "            print('             action:',num_to_act[action])  \n",
    "            print ('             **' )\n",
    "            print('reward:',reward,'new observation:',observation)\n",
    "            print('')\n",
    "            print('new internal (Mu) state:', ag.current_internal_state)\n",
    "            print('')\n",
    "            print('============================')\n",
    "\n",
    "\n",
    "        results[i].append(total_reward)\n",
    "\n",
    "\n",
    "print(\"results_b\", results_b)\n",
    "print(\"time_b\",efficiency_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af08117",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## MULTIVARIATE DATA GATHERING ################################################\n",
    "tx=time.time()\n",
    "loops=100\n",
    "\n",
    "results={1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[],9:[],10:[]}\n",
    "comptime={1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[],9:[],10:[]}\n",
    "for i in range(1,7):\n",
    "    \n",
    "        for j in range(loops):\n",
    "\n",
    "            planning_depth=i\n",
    "            partitioning_chunk_number=1000\n",
    "            exp_vorfaktoren=[1,-1]\n",
    "            exp_weights=[0.5,-0.5]\n",
    "            initial_theta=[0.5,0.5]\n",
    "            initial_observation=0\n",
    "            modes=['discrete_optimized','naive','cheating']\n",
    "\n",
    "            ###############    Environment\n",
    "            t0 = time.time()\n",
    "            e.reset()\n",
    "            initial_observation=e.current_state\n",
    "            observation=initial_observation\n",
    "            state=e.current_state\n",
    "\n",
    "            ##############     Agent \n",
    "\n",
    "            ag=Multi_Variate_agent(environment=e, planning_depth=planning_depth, partitioning_chunk_number=partitioning_chunk_number,agent_mode='cheating')\n",
    "            ag.reset()\n",
    "            mo_x_map,mo_M,mo_F,mo_G,mo_reachables,mo_value_function_zero,mo_all_thetas=ag.pre_planning(exp_vorfaktoren=exp_vorfaktoren,exp_weights=exp_weights, initial_theta=initial_theta,initial_observation=initial_observation,initial_wealth=0)\n",
    "            mo_value_func,mo_action_func,mo_q_func,vf2=ag.value_iteration()\n",
    "            t1 = time.time()\n",
    "            statesused=0\n",
    "            for i in range(planning_depth+1):\n",
    "                statesused+=len(mo_reachables[i])\n",
    "#             print(\"depth:\",planning_depth, \"states used:\", statesused)\n",
    "#             print(\"time needed:\", t1-t0)\n",
    "            comptime[i].append(t1-t0)\n",
    "\n",
    "            ##############       Simulation\n",
    "            total_reward=0\n",
    "            for t in range(planning_depth):\n",
    "\n",
    "#                 print('t=',t)\n",
    "#                 print('------')\n",
    "#                 print('state:',state,e.states[state])\n",
    "#                 print('last_observation:',observation)\n",
    "#                 print('-------------------------------')\n",
    "#                 print('')\n",
    "#                 print('current internal state:',ag.current_internal_state)\n",
    "#                 print('')\n",
    "\n",
    "                # agent select the action\n",
    "                action,value_of_action=ag.do_action()\n",
    "\n",
    "                #environment feedback\n",
    "                t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "                total_reward+=reward\n",
    "\n",
    "                # agent update\n",
    "                new_x=ag.update_agent(new_observation=observation) \n",
    "\n",
    "\n",
    "#                 print('             action:',num_to_act[action])  \n",
    "#                 print ('             **' )\n",
    "#                 print('reward:',reward,'new observation:',observation)\n",
    "#                 print('')\n",
    "#                 print('new internal state:', ag.current_internal_state)\n",
    "#                 print('')\n",
    "#                 print('============================')\n",
    "#                 print('total_reward', total_reward)\n",
    "\n",
    "            results[i].append(total_reward)\n",
    "        print(i)\n",
    "\n",
    "print(\"results\", results)\n",
    "print(\"=================================\")\n",
    "print(\"time\",comptime)\n",
    "\n",
    "############################## BAUERLE DATA GATHERING ################################################\n",
    "\n",
    "\n",
    "results_b={1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[],9:[],10:[]}\n",
    "comptime_b={1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[],9:[],10:[]}\n",
    "\n",
    "for i in range(1,7):\n",
    "    for j in range(loops):\n",
    "        total_reward=0\n",
    "        max_depth=i\n",
    "        planning_depth=max_depth\n",
    "        exp_vorfaktoren=[1,-1]\n",
    "        exp_weights=[0.5,-0.5]\n",
    "        initial_Mu=[0.5,0.5]\n",
    "        initial_observation=0\n",
    "        initial_wealth=0\n",
    "\n",
    "        ta=time.time()\n",
    "        ###############    Environment\n",
    "\n",
    "        e.reset()\n",
    "        initial_observation=e.current_state\n",
    "        observation=initial_observation\n",
    "        state=e.current_state\n",
    "        ##############       Agent\n",
    "        # Create the agent\n",
    "        ag=Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=2,max_iterations=max_depth)\n",
    "\n",
    "\n",
    "        # Run its value iteration\n",
    "        b_S,b_reachables,b_map,b_q_func,b_value_function,b_action_func=ag.continious_optimized_planning(initial_mu_state=initial_Mu,initial_observation=initial_observation,initial_wealth=0,exp_weights=exp_weights,exp_vorfaktoren=exp_vorfaktoren)\n",
    "        statesused=0\n",
    "        for i in range(max_depth+1):\n",
    "            statesused+=len(b_reachables[i])\n",
    "#         print(\"depth:\",max_depth, \"states used:\", statesused)\n",
    "        # reset the agent \n",
    "        ag.ch_reset(initial_mu_state=initial_Mu,initial_observation=initial_observation,initial_wealth=initial_wealth)\n",
    "\n",
    "        tb=time.time()\n",
    "#         print(\"time needed:\", tb-ta)\n",
    "        comptime_b[i].append(tb-ta)\n",
    "\n",
    "        #############         Simulation\n",
    "\n",
    "        for t in range(planning_depth):\n",
    "\n",
    "#             print('t=',t)\n",
    "#             print('------')\n",
    "#             print('state:',state,e.states[state])\n",
    "#             print('last_observation:',observation)\n",
    "#             print('-------------------------------')\n",
    "#             print('')\n",
    "#             print('current internal state:',ag.current_internal_state)\n",
    "#             print('')\n",
    "\n",
    "            # agent select the action\n",
    "            action,value_of_action,_=ag.ch_do_action()\n",
    "\n",
    "            #environment feedback\n",
    "            t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "            total_reward+=reward\n",
    "\n",
    "            # agent update\n",
    "            new_x=ag.ch_update_agent(new_observation=observation) \n",
    "\n",
    "#             print('             action:',num_to_act[action])  \n",
    "#             print ('             **' )\n",
    "#             print('reward:',reward,'new observation:',observation)\n",
    "#             print('')\n",
    "#             print('new internal (Mu) state:', ag.current_internal_state)\n",
    "#             print('')\n",
    "#             print('============================')\n",
    "\n",
    "\n",
    "        results_b[i].append(total_reward)\n",
    "    print(i)\n",
    "\n",
    "\n",
    "print(\"results_b\", results_b)\n",
    "print(\"===============\")\n",
    "print(\"time_b\",comptime_b)\n",
    "ty=time.time()\n",
    "print(\"----------------------\")\n",
    "print(\"total time needed for\", loops, \"loops\", ty-tx, \"seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadbbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## BAUERLE DATA GATHERING ################################################\n",
    "\n",
    "\n",
    "results_b={1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[],9:[],10:[]}\n",
    "efficiency_b={1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[],9:[],10:[]}\n",
    "\n",
    "for i in range(1,9):\n",
    "    for j in range(loops):\n",
    "        total_reward=0\n",
    "        max_depth=i\n",
    "        planning_depth=max_depth\n",
    "        exp_vorfaktoren=[1,-1]\n",
    "        exp_weights=[0.5,-0.5]\n",
    "        initial_Mu=[0.5,0.5]\n",
    "        initial_observation=0\n",
    "        initial_wealth=0\n",
    "\n",
    "        ta=time.time()\n",
    "        ###############    Environment\n",
    "\n",
    "        e.reset()\n",
    "        initial_observation=e.current_state\n",
    "        observation=initial_observation\n",
    "        state=e.current_state\n",
    "        ##############       Agent\n",
    "        # Create the agent\n",
    "        ag=Bauerle_Rieder_agent(environment=e, num_of_Mu_chunks=2,max_iterations=max_depth)\n",
    "\n",
    "\n",
    "        # Run its value iteration\n",
    "        b_S,b_reachables,b_map,b_q_func,b_value_function,b_action_func=ag.continious_optimized_planning(initial_mu_state=initial_Mu,initial_observation=initial_observation,initial_wealth=0,exp_weights=exp_weights,exp_vorfaktoren=exp_vorfaktoren)\n",
    "        statesused=0\n",
    "        for i in range(max_depth+1):\n",
    "            statesused+=len(b_reachables[i])\n",
    "#         print(\"depth:\",max_depth, \"states used:\", statesused)\n",
    "        # reset the agent \n",
    "        ag.ch_reset(initial_mu_state=initial_Mu,initial_observation=initial_observation,initial_wealth=initial_wealth)\n",
    "\n",
    "        tb=time.time()\n",
    "#         print(\"time needed:\", tb-ta)\n",
    "        efficiency_b[i].append(tb-ta)\n",
    "\n",
    "        #############         Simulation\n",
    "\n",
    "        for t in range(planning_depth):\n",
    "\n",
    "#             print('t=',t)\n",
    "#             print('------')\n",
    "#             print('state:',state,e.states[state])\n",
    "#             print('last_observation:',observation)\n",
    "#             print('-------------------------------')\n",
    "#             print('')\n",
    "#             print('current internal state:',ag.current_internal_state)\n",
    "#             print('')\n",
    "\n",
    "            # agent select the action\n",
    "            action,value_of_action,_=ag.ch_do_action()\n",
    "\n",
    "            #environment feedback\n",
    "            t1,t2,state,reward,observation=e.step(num_to_act[action])\n",
    "            total_reward+=reward\n",
    "\n",
    "            # agent update\n",
    "            new_x=ag.ch_update_agent(new_observation=observation) \n",
    "\n",
    "#             print('             action:',num_to_act[action])  \n",
    "#             print ('             **' )\n",
    "#             print('reward:',reward,'new observation:',observation)\n",
    "#             print('')\n",
    "#             print('new internal (Mu) state:', ag.current_internal_state)\n",
    "#             print('')\n",
    "#             print('============================')\n",
    "\n",
    "\n",
    "        results_b[i].append(total_reward)\n",
    "\n",
    "\n",
    "\n",
    "print(\"results_b\", results_b)\n",
    "print(\"time_b\",efficiency_b)\n",
    "ty=time.time()\n",
    "print(\"===============\")\n",
    "#print(\"total time needed for\", loops, \"loops\", ty-tx, \"seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0184d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
